import streamlit as st
import os
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.tools import DuckDuckGoSearchResults
from elevenlabs.client import ElevenLabs
from serpapi import GoogleSearch
import requests
import time

st.set_page_config(page_title="RAG Chatbot", page_icon="", layout="centered")

google_api_key = st.secrets.get("GOOGLE_API_KEY")
elevenlabs_api_key = st.secrets.get("ELEVENLABS_API_KEY")
serp_api_key = st.secrets.get("SERPAPI_KEY")

os.environ["GOOGLE_API_KEY"] = google_api_key

@st.cache_resource
def load_embeddings():
    return GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

embeddings = load_embeddings()  # Cached and will not reload on button click

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

#Directory
scripts_dir = "https://api.github.com/repos/ardatscn/RAG-Leadership-Coach-Turkish/contents/video_scripts"
response = requests.get(scripts_dir, auth=("ardatscn", "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"))

all_texts = []
files = response.json() 
a = 0
for file in files:
  fname = file['name']
  raw_url = file["download_url"]  # Get the raw URL of the file
  
  # Read the content of the file
  file_response = requests.get(raw_url)
  file_content = file_response.text  # Convert response to text
  splitted_text = text_splitter.split_text(file_content)
  all_texts.extend([(text, fname) for text in splitted_text])
    
txts, sources = zip(*all_texts)

@st.cache_resource
def load_vectors():
    vector_store = FAISS.from_texts(txts, embeddings, metadatas=[{"source": src} for src in sources])
    return vector_store

vector_store = load_vectors()
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash",  temperature=0)

retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 5})

system_prompt = (
    "Soruyu yan覺tlamak i癟in aa覺daki talimatlar覺 kullan覺n. "
    "Yan覺t T羹rk癟e olmal覺d覺r."
    "Cevap salanan balamda yoksa, zg羹n羹m cevap bulunamad覺.. yan覺t覺n覺 verin. "
    "Referans ald覺覺n kayna覺 cevab覺nda belirtmelisin."
    "Bir ko癟mu ve tavsiye veriyormu gibi konu."
    "Context: {context}"
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, prompt)
chain = create_retrieval_chain(retriever, question_answer_chain)

def search_online(query):
    params = {
        "q": query,
        "hl": "tr",
        "gl": "tr",
        "api_key": serp_api_key
    }

    search = GoogleSearch(params)
    results = search.get_dict()

    search_results = results.get("organic_results", [])
    return [(res.get("title", "No Title"), res.get("link", "#"), res.get("snippet", "No Snippet")) for res in search_results[:5]]
    
@st.cache_data
def search_online_cached(query):
    return search_online(query)

def query_rag(query):
    response = chain.invoke({"input": query})
    answer = response["answer"]
    references = {doc.metadata["source"].replace(".txt", "") for doc in response["context"]}

    if "zg羹n羹m cevap bulunamad覺" in answer:
        st.subheader("Eksik Veri! 襤te 襤nternette Bulunan Sonu癟lar:")
        result = search_online_cached(query)
        for title, link, snippet in result:
            st.markdown(f"**[{title}]({link})**")
            st.write(f"{snippet}")
            
    else:
        st.success(answer)
        st.success(references)

st.title("Leadership Coach")
st.write("Sorular覺n覺z 'Tecr羹be Konuuyor' YouTube Oynat覺m Listesinden Yan覺tlan覺r.")
query = st.text_input("Sorunuzu Sorun:", placeholder="rnek: Liderlerin ortak 繹zellikleri nelerdir?")

if st.button("Cevap Al"):
    if query:
        with st.spinner("Cevap Bekleniyor.."):
            query_rag(query)
            # st.success(answer)
            # st.success(references)

